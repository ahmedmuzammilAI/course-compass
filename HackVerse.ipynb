{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e4lcZikQ4WI8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install numpy pandas gradio scikit-learn streamlit roadmapper streamlit_webrtc\n",
        "!pip install faiss-gpu\n",
        "!pip install pyngrok\n",
        "!ngrok authtoken 24HGqrf1jrhD6NNVh9tsopK5zJ1_4bbEXFBUbD8yAECAyrP9K\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai langchain-google-genai streamlit streamlit-chat\n",
        "!pip install google-generativeai langchain-google-genai streamlit\n",
        "!pip install -U langchain openai chromadb langchainhub bs4\n",
        "!pip install langchain openai sounddevice\n",
        "!sudo apt-get install libportaudio2\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "1p4ZwwDUP0J1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for asr\n",
        "!pip install --upgrade pip networkx plotly\n",
        "!pip install --upgrade transformers accelerate datasets[audio]\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "yF5etxunnjsR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uxOzOGL7Mrwk"
      },
      "outputs": [],
      "source": [
        "!streamlit run hackverse.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RF1P_2DaMrwl"
      },
      "outputs": [],
      "source": [
        "# !npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whYiEm_GaD8X",
        "outputId": "ea169424-829d-4815-8f89-cd13946c2ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://cc51-34-125-40-37.ngrok-free.app\" -> open your app!\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(\" * ngrok tunnel \\\"{}\\\" -> open your app!\".format(public_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tesing udemy api"
      ],
      "metadata": {
        "id": "jH3sXZuKEe8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyudemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HTmw2OF_PCd",
        "outputId": "7e0727ae-ccbd-437b-db24-f6c8d09b577e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyudemy\n",
            "  Downloading pyudemy-1.0.8-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from pyudemy) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->pyudemy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->pyudemy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->pyudemy) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.28.1->pyudemy) (2023.11.17)\n",
            "Downloading pyudemy-1.0.8-py3-none-any.whl (4.3 kB)\n",
            "Installing collected packages: pyudemy\n",
            "Successfully installed pyudemy-1.0.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyudemy import Udemy\n",
        "# udemy = Udemy(<CLIENT_ID>, <CLIENT_SECRET>)"
      ],
      "metadata": {
        "id": "v6q4QKk9FLs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SMXQMBRRJmMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import urllib.request\n",
        "import urllib.request as ur\n",
        "s = ur.urlopen(\"https://www.udacity.com/public-api/v0/courses\")\n",
        "sl = s.read()\n",
        "json_response=json.loads(sl)\n",
        "for course in json_response['courses']:\n",
        "    print(course['title']+':'+course['homepage'])\n",
        "# print(sl)\n",
        "# with urllib.request.urlopen(\"https://www.udacity.com/public-api/v0/courses\") as url:\n",
        "  #   # s = url.read()\n",
        "  # # response=urllib.urlopen('https://www.udacity.com/public-api/v0/courses')\n",
        "  # json_response=json.loads(url.read())\n",
        "  # for course in json_response['courses']:\n",
        "  #   print(course['title']+':'+course['homepage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "mgNB0nPVJY0F",
        "outputId": "da6f5a0b-4f0c-41c5-f7dc-3e5c47654fba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-cd1cb770dc9d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.udacity.com/public-api/v0/courses\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mjson_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Create a dictionary of courses and their details\n",
        "courses = {\n",
        "    \"Introduction to Programming Using Python (Beginner)\": {\n",
        "        \"Description\": \"Learn fundamental programming concepts like variables, loops, and functions.\",\n",
        "        \"Level\": \"Beginner\"\n",
        "    },\n",
        "    \"Data Structures and Algorithms (Intermediate)\": {\n",
        "        \"Description\": \"Explore common data structures like lists, stacks, queues, and trees.\",\n",
        "        \"Level\": \"Intermediate\"\n",
        "    },\n",
        "    \"Object-Oriented Programming with Java (Intermediate)\": {\n",
        "        \"Description\": \"Learn the principles of object-oriented programming with Java.\",\n",
        "        \"Level\": \"Intermediate\"\n",
        "    },\n",
        "    \"Web Development with JavaScript and HTML/CSS (Beginner-Intermediate)\": {\n",
        "        \"Description\": \"Learn basics of HTML, CSS, and JavaScript for web development.\",\n",
        "        \"Level\": \"Beginner-Intermediate\"\n",
        "    },\n",
        "    \"Machine Learning with Python (Intermediate-Advanced)\": {\n",
        "        \"Description\": \"Explore supervised learning algorithms and machine learning libraries.\",\n",
        "        \"Level\": \"Intermediate-Advanced\"\n",
        "    },\n",
        "    \"App Development with Swift (Beginner-Intermediate)\": {\n",
        "        \"Description\": \"Learn basics of Swift programming and app development for iOS.\",\n",
        "        \"Level\": \"Beginner-Intermediate\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges to the graph\n",
        "for course, details in courses.items():\n",
        "    G.add_node(course, description=details[\"Description\"], level=details[\"Level\"])\n",
        "\n",
        "# Define edges indicating progression or relationship between courses\n",
        "edges = [\n",
        "    (\"Introduction to Programming Using Python (Beginner)\", \"Data Structures and Algorithms (Intermediate)\"),\n",
        "    (\"Data Structures and Algorithms (Intermediate)\", \"Object-Oriented Programming with Java (Intermediate)\"),\n",
        "    (\"Data Structures and Algorithms (Intermediate)\", \"Machine Learning with Python (Intermediate-Advanced)\"),\n",
        "    (\"Object-Oriented Programming with Java (Intermediate)\", \"App Development with Swift (Beginner-Intermediate)\"),\n",
        "    (\"Web Development with JavaScript and HTML/CSS (Beginner-Intermediate)\", \"App Development with Swift (Beginner-Intermediate)\")\n",
        "]\n",
        "\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Create a layout for the graph\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw(G, pos, with_labels=True, node_size=1500, node_color='skyblue', font_weight='bold', font_size=10, edge_color='gray')\n",
        "node_desc = {node: details['description'] for node, details in courses.items()}\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): '' for u, v in edges}, font_color='red', font_size=8)\n",
        "nx.draw_networkx_labels(G, pos, labels=node_desc, font_size=8, font_color='black', verticalalignment='bottom')\n",
        "plt.title('Career Choices and Progression Path')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rnMMgoezERqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from roadmapper.roadmap import Roadmap\n",
        "# from roadmapper.timelinemode import TimelineMode\n",
        "\n",
        "# my_roadmap = Roadmap(width=500, height=300)\n",
        "# my_roadmap.set_title(\"My Roadmap\")\n",
        "# my_roadmap.set_timeline(mode=TimelineMode.MONTHLY, start=\"2022-11-14\", number_of_items=6)\n",
        "\n",
        "# group = my_roadmap.add_group(\"Development\")\n",
        "# group.add_task(\"Activity 1\", \"2022-12-01\", \"2023-02-10\")\n",
        "# group.add_task(\"Activity 2\", \"2023-01-11\", \"2023-03-20\")\n",
        "# group.add_task(\"Activity 3\", \"2023-01-21\", \"2023-06-30\")\n",
        "\n",
        "# my_roadmap.set_footer(\"Generated by Roadmapper\")\n",
        "# my_roadmap.draw()\n",
        "# my_roadmap.save(\"my_roadmap.png\")"
      ],
      "metadata": {
        "id": "h5oatljX1pKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# backup codes\n",
        "hackverse.py"
      ],
      "metadata": {
        "id": "VIfhE5ixObqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from streamlit_chat import message\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "# Load data\n",
        "data = pd.read_csv(r'coursera_courses.csv')\n",
        "\n",
        "# TF-IDF vectorization\n",
        "course_corpus = data['course_title']\n",
        "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=5)\n",
        "X = vectorizer.fit_transform(course_corpus)\n",
        "\n",
        "# Convert sparse matrix to numpy array\n",
        "X_array = np.float32(X.toarray())\n",
        "\n",
        "# Create Faiss index\n",
        "index = faiss.IndexFlatL2(X_array.shape[1])\n",
        "index.add(X_array)\n",
        "\n",
        "#for LLM gemini\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCvXu33gltO3ZEL5WRjqSyrl4ANgDeO84o\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        # SystemMessagePromptTemplate.from_template(\n",
        "        #     \"you are a course recommender, you ask the candidates a few questions to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced.\"\n",
        "        # ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses. suggest atleat one course from knowvationlearnings.in\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "def create_and_plot_graph(recommendations):\n",
        "    # Create a directed graph\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Define courses for different levels\n",
        "    # beginner_courses = recommendations['Beginner']\n",
        "\n",
        "    # intermediate_courses = recommendations['Intermediate']\n",
        "\n",
        "    # advanced_courses = recommendations['Advanced']\n",
        "\n",
        "    beginner_courses = [\n",
        "        (\"Data Science from Johns Hopkins University\", \"Fractal Data Science from Fractal Analytics\"),\n",
        "    (\"Data Science from Johns Hopkins University\", \"What is Data Science? from IBM\"),\n",
        "    (\"IBM Data Science from IBM\", \"SQL for Data Science from University of California, Davis\"),\n",
        "\n",
        "\n",
        "    (\"IBM Data Science from IBM\", \"Data Science Math Skills from Duke University\"),\n",
        "    (\"Tools for Data Science from IBM\", \"Practical Data Science with MATLAB from MathWorks\")# Add more beginner courses as needed\n",
        "    ]\n",
        "\n",
        "    intermediate_courses = [\n",
        "        (\"Data Science with Databricks for Data Analysts from Databricks\", \"Genomic Data Science from Johns Hopkins University\"),\n",
        "        (\"IBM Data Science from IBM\", \"Introduction to Data Science from IBM\"),\n",
        "    (\"IBM Data Science from IBM\", \"Tools for Data Science from IBM\"),\n",
        "    (\"IBM Data Science from IBM\", \"Applied Data Science from IBM\")# Add more intermediate courses as needed\n",
        "    ]\n",
        "\n",
        "    advanced_courses = [\n",
        "        (\"Genomic Data Science from Johns Hopkins University\", \"Foundations of Data Science from Google\"),\n",
        "        (\"IBM Data Science from IBM\", \"Executive Data Science from Johns Hopkins University\"),\n",
        "    (\"IBM Data Science from IBM\", \"Data Science Methodology from IBM\")# Add more advanced courses as needed\n",
        "    ]\n",
        "    # Add nodes and edges to the graph\n",
        "    G.add_edges_from(beginner_courses)\n",
        "    G.add_edges_from(intermediate_courses)\n",
        "    G.add_edges_from(advanced_courses)\n",
        "\n",
        "    # Positioning nodes in the graph\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "    # Create a Plotly figure\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add nodes to the figure\n",
        "    for node in G.nodes:\n",
        "        x, y = pos[node]\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[x],\n",
        "            y=[y],\n",
        "            mode=\"markers\",\n",
        "            marker=dict(size=16, color=\"skyblue\"),\n",
        "            text=node,\n",
        "            hoverinfo=\"text\",\n",
        "            name=node,\n",
        "        ))\n",
        "\n",
        "    # Add edges to the figure\n",
        "    for edge in G.edges:\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[x0, x1, None],\n",
        "            y=[y0, y1, None],\n",
        "            mode=\"lines\",\n",
        "            line=dict(color=\"gray\", width=0.5),\n",
        "            hoverinfo=\"none\",\n",
        "        ))\n",
        "\n",
        "    # Customize plot appearance\n",
        "    fig.update_layout(\n",
        "        title_text=\"Data Science Courses Flow Diagram\",\n",
        "        title_x=0.5,\n",
        "        showlegend=False,\n",
        "        hovermode=\"closest\",\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "# Function to recommend courses by difficulty\n",
        "def recommend_courses_by_difficulty(title):\n",
        "    search_text = [title]\n",
        "    search_text_vector = vectorizer.transform(search_text)\n",
        "    search_text_vector_array = np.float32(search_text_vector.toarray())\n",
        "    distances, indices = index.search(search_text_vector_array, 15)\n",
        "\n",
        "    recommendations = {'Beginner': [], 'Intermediate': [], 'Advanced': []}\n",
        "\n",
        "    for i in range(15):\n",
        "        course_title = data['course_title'][indices[0][i]]\n",
        "        organization = data['course_organization'][indices[0][i]]\n",
        "        difficulty = data['course_difficulty'][indices[0][i]]\n",
        "        link = data['course_url'][indices[0][i]]\n",
        "\n",
        "        recommendation = f\"[{course_title}]({link}) from {organization}\\nDifficulty: {difficulty}\"\n",
        "\n",
        "        # Add the recommendation to the corresponding difficulty level\n",
        "        recommendations[difficulty].append(recommendation)\n",
        "\n",
        "        # Break once we have 3 recommendations for each difficulty level\n",
        "        if all(len(recommendations[level]) >= 3 for level in ['Beginner', 'Intermediate', 'Advanced']):\n",
        "            break\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "def langchain_conversation(user_input):\n",
        "    # st.write(\"in lgconv\")\n",
        "    messages = []\n",
        "\n",
        "    # Append user input to messages\n",
        "    # messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Ask a new user prompt\n",
        "    # prompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n",
        "    # messages.append(prompt)\n",
        "    # st.write(\"gening resp\")\n",
        "    try:\n",
        "      response = conversation({\"question\": user_input})\n",
        "      # st.write(response[\"chat_history\"][1])\n",
        "      return response\n",
        "\n",
        "    except Exception as e:\n",
        "    # Catching any other exceptions\n",
        "      print(\"An error occurred:\", e)\n",
        "\n",
        "# Streamlit App\n",
        "def get_text():\n",
        "    input_text = st.text_input(\"You: \", \"Hello, how are you?\", key=\"input\")\n",
        "    return input_text\n",
        "# user_input = get_text()\n",
        "\n",
        "st.set_page_config(page_title=\"Course Compass\", page_icon=\":robot:\")\n",
        "st.header(\"Course Compass - Gemini AI + vector DB\")\n",
        "st.sidebar.title(\"Select Mode\")\n",
        "\n",
        "# Sidebar radio buttons for mode selection\n",
        "selected_mode = st.sidebar.radio(\"Select Mode\", [ \"AI Course Strategizer\", \"AI Course Counsellor\"])\n",
        "\n",
        "def talk():\n",
        "  st.subheader(\"AI Counsellor Conversation:\")\n",
        "  uploaded_file = \"/content/mona_talking.mp4\"\n",
        "  if uploaded_file is not None:\n",
        "      # video_bytes = uploaded_file.read()\n",
        "      st.video(uploaded_file, format='video/mp4')\n",
        "\n",
        "  if st.button(\"Start Conversation\"):\n",
        "    st.subheader(\"Hold space bar while you speak..\")\n",
        "    st.subheader(\"Running ASR(Automatic Speech Recog) on your voice\")\n",
        "    st.subheader(\"Parsing the input and passing to the LLM model\")\n",
        "    st.subheader(\"LLM Model Output spoken back by AI counsellor\")\n",
        "    st.subheader(\"OUTPUT: Curated learning path recommendation and Roadmap\")\n",
        "\n",
        "def chat():\n",
        "  # user_input = st.text_input(\"search courses based on keywords:\", \"\")\n",
        "\n",
        "    # Add logic to ask questions based on user's interests and generate recommendations\n",
        "  # if st.button(\"Ask AI\"):\n",
        "  #       # Add logic to generate AI responses and recommendations based on user input\n",
        "  #       st.subheader(\"AI's Response:\")\n",
        "  #       st.text(\"AI's response goes here.\")\n",
        "\n",
        "# Course recommendation feature with search bar\n",
        "  image = open('/content/diag_hackverse.jpg', 'rb').read()\n",
        "  st.image(image, caption='The backend Architecture')\n",
        "\n",
        "  st.subheader(\"Course Recommendation from data:\")\n",
        "  user_input = st.text_input(\"Search for a course:\", \"data science\")\n",
        "  if st.button(\"Search using FAISS vector Search\"):\n",
        "      # Add logic to display course recommendations based on the search query\n",
        "      recommended_courses = recommend_courses_by_difficulty(user_input)\n",
        "\n",
        "      for difficulty, recommendations in recommended_courses.items():\n",
        "          st.subheader(f\"{difficulty} Courses:\")\n",
        "          for recommendation in recommendations:\n",
        "              st.markdown(recommendation, unsafe_allow_html=True)\n",
        "\n",
        "      # if st.button(\"show roadmap\"):\n",
        "      st.title(\"Data Science Courses Flow Diagram\")\n",
        "      st.plotly_chart(create_and_plot_graph(recommended_courses))\n",
        "\n",
        "  if \"generated\" not in st.session_state:\n",
        "\n",
        "      st.session_state[\"generated\"] = []\n",
        "\n",
        "  if \"past\" not in st.session_state:\n",
        "      st.session_state[\"past\"] = []\n",
        "\n",
        "  user_input2 = st.text_input(\"ask gemini:\", \"i wanna be a data scientist\")\n",
        "\n",
        "  if user_input2 and st.button(\"Ask Gemini AI!\"):\n",
        "      # output = chain.run(input=user_input)\n",
        "      output = langchain_conversation(user_input2)\n",
        "      st.session_state.past.append(user_input2)\n",
        "      st.session_state.generated.append(output[\"chat_history\"][1].content)\n",
        "      # st.write(output[\"chat_history\"][1].content)\n",
        "  if st.session_state[\"generated\"]:\n",
        "\n",
        "      for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n",
        "          message(st.session_state[\"generated\"][i], key=str(i))\n",
        "          message(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
        "# Display latest curated blogs\n",
        "# st.subheader(\"Latest Curated Blogs:\")\n",
        "# Add logic to display the latest curated blogs based on the backend model\n",
        "\n",
        "# Marquee text\n",
        "# st.markdown(\"<p style='color:red; font-size:20px;'>Selective courses curated just for you from 6000 courses</p>\", unsafe_allow_html=True)\n",
        "\n",
        "# Display recommendations based on the selected mode\n",
        "if selected_mode == \"AI Course Counsellor\":\n",
        "    # Display AI avatar and conversation\n",
        "    talk()\n",
        "    # Add logic to display AI avatar conversation and recommendations based on the conversation\n",
        "\n",
        "elif selected_mode == \"AI Course Strategizer\":\n",
        "    # Input box for user to enter text\n",
        "    chat()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "69zdIyY4OaIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gemini.py"
      ],
      "metadata": {
        "id": "kK7ghhOpOjny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Python file to serve as the frontend\"\"\"\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCvXu33gltO3ZEL5WRjqSyrl4ANgDeO84o\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        # SystemMessagePromptTemplate.from_template(\n",
        "        #     \"you are a course recommender, you ask the candidates a few questions to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced.\"\n",
        "        # ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "def langchain_conversation(user_input):\n",
        "    messages = []\n",
        "\n",
        "    # Append user input to messages\n",
        "    # messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Ask a new user prompt\n",
        "    # prompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n",
        "    # messages.append(prompt)\n",
        "\n",
        "    # Send to OpenAI for conversation\n",
        "\n",
        "    response = conversation({\"question\": user_input})\n",
        "    print(response[\"chat_history\"][1])\n",
        "    return response\n",
        "\n",
        "# while True:\n",
        "#     langchain_conversation(input(\"user_input \"))\n",
        "\n",
        "# def load_chain():\n",
        "#     \"\"\"Logic for loading the chain you want to use should go here.\"\"\"\n",
        "#     # llm = OpenAI(temperature=0)\n",
        "#     conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "#     # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "#     return conversation({\"question\": \"hi\"})\n",
        "#     # return chain\n",
        "\n",
        "# chain = load_chain()\n",
        "\n",
        "# From here down is all the StreamLit UI.\n",
        "st.set_page_config(page_title=\"Course Compass\", page_icon=\":robot:\")\n",
        "st.header(\"Course Compass - Gemini AI + vector DB\")\n",
        "\n",
        "if \"generated\" not in st.session_state:\n",
        "    st.session_state[\"generated\"] = []\n",
        "\n",
        "if \"past\" not in st.session_state:\n",
        "    st.session_state[\"past\"] = []\n",
        "\n",
        "\n",
        "def get_text():\n",
        "    input_text = st.text_input(\"You: \", \"Hello, how are you?\", key=\"input\")\n",
        "    return input_text\n",
        "\n",
        "\n",
        "user_input = get_text()\n",
        "\n",
        "if user_input and st.button(\"ask!\"):\n",
        "    # output = chain.run(input=user_input)\n",
        "    output = langchain_conversation(user_input)\n",
        "    st.session_state.past.append(user_input)\n",
        "    st.session_state.generated.append(output[\"chat_history\"][1].content)\n",
        "    # st.write(output[\"chat_history\"][1].content)\n",
        "if st.session_state[\"generated\"]:\n",
        "\n",
        "    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n",
        "        message(st.session_state[\"generated\"][i], key=str(i))\n",
        "        message(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")"
      ],
      "metadata": {
        "id": "I5BnGoIKPbd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "integration testing code backup"
      ],
      "metadata": {
        "id": "_4LaSWiiQVjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(r'coursera_courses.csv')\n",
        "\n",
        "# TF-IDF vectorization\n",
        "course_corpus = data['course_title']\n",
        "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 3), min_df=5)\n",
        "X = vectorizer.fit_transform(course_corpus)\n",
        "\n",
        "# Convert sparse matrix to numpy array\n",
        "X_array = np.float32(X.toarray())\n",
        "\n",
        "# Create Faiss index\n",
        "index = faiss.IndexFlatL2(X_array.shape[1])\n",
        "index.add(X_array)\n",
        "\n",
        "#for LLM gemini\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCvXu33gltO3ZEL5WRjqSyrl4ANgDeO84o\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        # SystemMessagePromptTemplate.from_template(\n",
        "        #     \"you are a course recommender, you ask the candidates a few questions to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced.\"\n",
        "        # ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "\n",
        "# Function to recommend courses by difficulty\n",
        "def recommend_courses_by_difficulty(title):\n",
        "    search_text = [title]\n",
        "    search_text_vector = vectorizer.transform(search_text)\n",
        "    search_text_vector_array = np.float32(search_text_vector.toarray())\n",
        "    distances, indices = index.search(search_text_vector_array, 15)\n",
        "\n",
        "    recommendations = {'Beginner': [], 'Intermediate': [], 'Advanced': []}\n",
        "\n",
        "    for i in range(15):\n",
        "        course_title = data['course_title'][indices[0][i]]\n",
        "        organization = data['course_organization'][indices[0][i]]\n",
        "        difficulty = data['course_difficulty'][indices[0][i]]\n",
        "        link = data['course_url'][indices[0][i]]\n",
        "\n",
        "        recommendation = f\"[{course_title}]({link}) from {organization}\\nDifficulty: {difficulty}\"\n",
        "\n",
        "        # Add the recommendation to the corresponding difficulty level\n",
        "        recommendations[difficulty].append(recommendation)\n",
        "\n",
        "        # Break once we have 3 recommendations for each difficulty level\n",
        "        if all(len(recommendations[level]) >= 3 for level in ['Beginner', 'Intermediate', 'Advanced']):\n",
        "            break\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Streamlit App\n",
        "st.set_page_config(page_title=\"Course Compass\", page_icon=\":robot:\")\n",
        "st.header(\"Course Compass - Gemini AI + vector DB\")\n",
        "st.sidebar.title(\"Select Mode\")\n",
        "\n",
        "# Sidebar radio buttons for mode selection\n",
        "selected_mode = st.sidebar.radio(\"Select Mode\", [\"Talk to AI\", \"Chat with AI\"])\n",
        "\n",
        "# Display latest curated blogs\n",
        "st.subheader(\"Latest Curated Blogs:\")\n",
        "# Add logic to display the latest curated blogs based on the backend model\n",
        "\n",
        "# Marquee text\n",
        "st.markdown(\"<p style='color:red; font-size:20px;'>Selective courses curated just for you from 6000 courses</p>\", unsafe_allow_html=True)\n",
        "\n",
        "# Display recommendations based on the selected mode\n",
        "if selected_mode == \"Talk to AI\":\n",
        "    # Display AI avatar and conversation\n",
        "    st.subheader(\"AI Avatar Conversation:\")\n",
        "    # Add logic to display AI avatar conversation and recommendations based on the conversation\n",
        "\n",
        "elif selected_mode == \"Chat with AI\":\n",
        "    # Input box for user to enter text\n",
        "    user_input = st.text_input(\"Chat with AI:\", \"\")\n",
        "\n",
        "    # Add logic to ask questions based on user's interests and generate recommendations\n",
        "    if st.button(\"Ask AI\"):\n",
        "        # Add logic to generate AI responses and recommendations based on user input\n",
        "        st.subheader(\"AI's Response:\")\n",
        "        st.text(\"AI's response goes here.\")\n",
        "\n",
        "# Course recommendation feature with search bar\n",
        "st.subheader(\"Course Recommendation:\")\n",
        "search_course_title = st.text_input(\"Search for a course:\", \"\")\n",
        "if st.button(\"Search\"):\n",
        "    # Add logic to display course recommendations based on the search query\n",
        "    recommended_courses = recommend_courses_by_difficulty(search_course_title)\n",
        "    for difficulty, recommendations in recommended_courses.items():\n",
        "        st.subheader(f\"{difficulty} Courses:\")\n",
        "        for recommendation in recommendations:\n",
        "            st.markdown(recommendation, unsafe_allow_html=True)\n",
        "def langchain_conversation(user_input):\n",
        "    messages = []\n",
        "\n",
        "    # Append user input to messages\n",
        "    # messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Ask a new user prompt\n",
        "    # prompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n",
        "    # messages.append(prompt)\n",
        "\n",
        "    # Send to OpenAI for conversation\n",
        "\n",
        "    response = conversation({\"question\": user_input})\n",
        "    print(response[\"chat_history\"][1])\n",
        "    return response\n",
        "\n",
        "# while True:\n",
        "#     langchain_conversation(input(\"user_input \"))\n",
        "\n",
        "# def load_chain():\n",
        "#     \"\"\"Logic for loading the chain you want to use should go here.\"\"\"\n",
        "#     # llm = OpenAI(temperature=0)\n",
        "#     conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "#     # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
        "#     return conversation({\"question\": \"hi\"})\n",
        "#     # return chain\n",
        "\n",
        "# chain = load_chain()\n",
        "\n",
        "# From here down is all the StreamLit UI.\n",
        "\n",
        "if \"generated\" not in st.session_state:\n",
        "    st.session_state[\"generated\"] = []\n",
        "\n",
        "if \"past\" not in st.session_state:\n",
        "    st.session_state[\"past\"] = []\n",
        "\n",
        "\n",
        "def get_text():\n",
        "    input_text = st.text_input(\"You: \", \"Hello, how are you?\", key=\"input\")\n",
        "    return input_text\n",
        "\n",
        "\n",
        "user_input = get_text()\n",
        "\n",
        "if user_input and st.button(\"ask!\"):\n",
        "    # output = chain.run(input=user_input)\n",
        "    output = langchain_conversation(user_input)\n",
        "    st.session_state.past.append(user_input)\n",
        "    st.session_state.generated.append(output[\"chat_history\"][1].content)\n",
        "    # st.write(output[\"chat_history\"][1].content)\n",
        "if st.session_state[\"generated\"]:\n",
        "\n",
        "    for i in range(len(st.session_state[\"generated\"]) - 1, -1, -1):\n",
        "        message(st.session_state[\"generated\"][i], key=str(i))\n",
        "        message(st.session_state[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IMcgdMsNQYy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "only Gemini in cell chatbot testing"
      ],
      "metadata": {
        "id": "IbJ-CGjhYyvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyCvXu33gltO3ZEL5WRjqSyrl4ANgDeO84o\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "# response = llm.invoke(\"Write a 5 line poem on AI\")\n",
        "\n",
        "# print(response.content)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        # SystemMessagePromptTemplate.from_template(\n",
        "        #     \"you are a course recommender, you ask the candidates a few questions to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced.\"\n",
        "        # ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)\n",
        "def langchain_conversation(user_input):\n",
        "    messages = []\n",
        "\n",
        "    # Append user input to messages\n",
        "    # messages.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Ask a new user prompt\n",
        "    # prompt = HumanMessage(content=\"Can you tell me about the LLMChain in LangChain?\")\n",
        "    # messages.append(prompt)\n",
        "\n",
        "    # Send to OpenAI for conversation\n",
        "\n",
        "    response = conversation({\"question\": user_input})\n",
        "    print(response[\"chat_history\"][1])\n",
        "    return response\n",
        "\n",
        "while True:\n",
        "    langchain_conversation(input(\"user_input \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X9ylboPcX8lT",
        "outputId": "54f60d4d-3195-4577-9094-683ec3eb62f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_input hi i wanna learn ML\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-16 20:46:31.837 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 6677.65ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "content='1. What are your primary interests and passions?\\n\\n2. What are your end goals or aspirations in terms of your career or personal development?\\n\\n3. What is your current skill level in the area of interest you specified? (Beginner, intermediate, advanced)\\n\\n4. Do you have any specific learning preferences or requirements? (e.g., online courses, self-paced learning, structured classes)\\n\\n5. Do you have any time constraints or limitations that we need to consider in recommending courses?\\n\\nBased on your responses, here are some courses that might align with your interests, end goals, and current skill level:\\n\\nCourse 1: Title - Difficulty Level - Brief Overview\\n\\nCourse 2: Title - Difficulty Level - Brief Overview\\n\\nCourse 3: Title - Difficulty Level - Brief Overview\\n\\nCourse 4: Title - Difficulty Level - Brief Overview\\n\\nCourse 5: Title - Difficulty Level - Brief Overview\\n\\nThese courses offer a range of difficulty levels, from beginner to advanced, and can help you progress towards your goals. Let me know if you have any questions or need further clarification on the courses recommended.'\n",
            "user_input i wanna become a good ML engineer\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: hi i wanna learn ML\n",
            "AI: 1. What are your primary interests and passions?\n",
            "\n",
            "2. What are your end goals or aspirations in terms of your career or personal development?\n",
            "\n",
            "3. What is your current skill level in the area of interest you specified? (Beginner, intermediate, advanced)\n",
            "\n",
            "4. Do you have any specific learning preferences or requirements? (e.g., online courses, self-paced learning, structured classes)\n",
            "\n",
            "5. Do you have any time constraints or limitations that we need to consider in recommending courses?\n",
            "\n",
            "Based on your responses, here are some courses that might align with your interests, end goals, and current skill level:\n",
            "\n",
            "Course 1: Title - Difficulty Level - Brief Overview\n",
            "\n",
            "Course 2: Title - Difficulty Level - Brief Overview\n",
            "\n",
            "Course 3: Title - Difficulty Level - Brief Overview\n",
            "\n",
            "Course 4: Title - Difficulty Level - Brief Overview\n",
            "\n",
            "Course 5: Title - Difficulty Level - Brief Overview\n",
            "\n",
            "These courses offer a range of difficulty levels, from beginner to advanced, and can help you progress towards your goals. Let me know if you have any questions or need further clarification on the courses recommended.\n",
            "Human: Imagine you are a course recommender, ask the candidates questions one by one to get his personal interests, end goals and current skill level and provide him with a curated list of courses alongside mentioning its difficulty level as beginner, intermediate and advanced. do not ask multiple questions at once. wait for the user to answer each question one by one and after 4-5 question provide him with a list of courses\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-16 20:46:56.472 200 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5890.84ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "content='1. What are your primary interests and passions?\\n\\n2. What are your end goals or aspirations in terms of your career or personal development?\\n\\n3. What is your current skill level in the area of interest you specified? (Beginner, intermediate, advanced)\\n\\n4. Do you have any specific learning preferences or requirements? (e.g., online courses, self-paced learning, structured classes)\\n\\n5. Do you have any time constraints or limitations that we need to consider in recommending courses?\\n\\nBased on your responses, here are some courses that might align with your interests, end goals, and current skill level:\\n\\nCourse 1: Title - Difficulty Level - Brief Overview\\n\\nCourse 2: Title - Difficulty Level - Brief Overview\\n\\nCourse 3: Title - Difficulty Level - Brief Overview\\n\\nCourse 4: Title - Difficulty Level - Brief Overview\\n\\nCourse 5: Title - Difficulty Level - Brief Overview\\n\\nThese courses offer a range of difficulty levels, from beginner to advanced, and can help you progress towards your goals. Let me know if you have any questions or need further clarification on the courses recommended.'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-039e940d9e72>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mlangchain_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user_input \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "GibhiZU6Zdnl",
        "outputId": "ac6be6ac-66f5-4c5c-aaa3-c5e41d06f7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: i wanna learn ML\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1ff841ea2b35>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-1ff841ea2b35>\u001b[0m in \u001b[0;36minteract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chatbot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         callback_manager = CallbackManager.configure(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/callbacks/manager.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mCallbackManager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mconfigured\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \"\"\"\n\u001b[0;32m-> 1301\u001b[0;31m         return _configure(\n\u001b[0m\u001b[1;32m   1302\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             \u001b[0minheritable_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/callbacks/manager.py\u001b[0m in \u001b[0;36m_configure\u001b[0;34m(callback_manager_cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m             callback_manager = callback_manager_cls(\n\u001b[0;32m-> 1776\u001b[0;31m                 \u001b[0mhandlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minheritable_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m                 \u001b[0minheritable_handlers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minheritable_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minheritable_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m                 \u001b[0mparent_run_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minheritable_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_run_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'handlers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oaaQ5pYPZeJu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}